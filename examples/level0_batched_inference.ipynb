{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Level 0: Batched MAP Shear Inference\n\nThis notebook demonstrates SHINE's **Level 0 sanity check** on a batch of 10 galaxies\nsharing the same true shear. Level 0 is the noiseless self-consistency test: when data\nis generated by the exact same forward model with effectively zero noise, the MAP\nestimate should recover the truth exactly.\n\nSince there is no noise, MAP (point estimation) is the natural choice -- full MCMC\nis unnecessary and much slower.\n\nSimulation parameters are matched to the\n[ngmix metacal example](https://github.com/esheldon/ngmix/blob/master/examples/metacal/metacal.py):\n\n| Parameter | Value | Source |\n|-----------|-------|--------|\n| Galaxy | Exponential, hlr=0.5\" | metacal `gal_hlr=0.5` |\n| PSF | Moffat, $\\beta$=2.5, FWHM=0.9\" | metacal `psf_fwhm=0.9` |\n| Pixel scale | 0.263\"/px | metacal `scale=0.263` |\n| Noise | $\\sigma = 10^{-6}$ | metacal `noise=1e-6` |\n| Shear | $g_1=0.01$, $g_2=0.00$ | metacal `shear_true=[0.01, 0.00]` |\n\n**What we do:**\n1. Generate 10 synthetic observations with the same shear\n2. Run MAP inference on each realization independently\n3. Check that MAP estimates match truth with negligible bias"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nfrom shine.config import (\n    ShineConfig,\n    ImageConfig,\n    NoiseConfig,\n    PSFConfig,\n    GalaxyConfig,\n    ShearConfig,\n    EllipticityConfig,\n    PositionConfig,\n    InferenceConfig,\n    MAPConfig,\n    DistributionConfig,\n)\nfrom shine.scene import SceneBuilder\nfrom shine.inference import Inference\nfrom shine.validation.simulation import generate_biased_observation\nfrom shine.validation.extraction import (\n    extract_convergence_diagnostics,\n    extract_shear_estimates,\n    check_convergence,\n)\nfrom shine.validation.bias_config import ConvergenceThresholds\nfrom shine.validation.statistics import compute_bias_single_point\n\n# Use 64-bit precision for accurate shear recovery\njax.config.update(\"jax_enable_x64\", True)\n\nprint(f\"JAX devices: {jax.devices()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Parameters matched to the [ngmix metacal example](https://github.com/esheldon/ngmix/blob/master/examples/metacal/metacal.py):\n",
    "- **Galaxy**: Exponential, hlr=0.5\" (metacal `gal_hlr=0.5`)\n",
    "- **PSF**: Moffat, $\\beta = 2.5$, FWHM=0.9\" (metacal `psf_fwhm=0.9, beta=2.5`)\n",
    "- **Pixel scale**: 0.263\"/px (metacal `scale=0.263`)\n",
    "- **Noise**: $\\sigma = 10^{-6}$ (metacal default `noise=1e-6`)\n",
    "- **Shear**: $g_1 = 0.01$, $g_2 = 0.00$ (metacal `shear_true=[0.01, 0.00]`)\n",
    "- **Position**: Fixed at center (metacal uses random subpixel offsets)\n",
    "\n",
    "**Note:** The metacal PSF has intrinsic ellipticity ($g_1^{\\rm PSF}=0.02$, $g_2^{\\rm PSF}=-0.01$).\n",
    "SHINE does not yet support PSF ellipticity, so we use a round Moffat PSF here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ground truth shear (matches metacal shear_true=[0.01, 0.00])\nG1_TRUE = 0.01\nG2_TRUE = 0.00\nN_BATCH = 10\n\nconfig = ShineConfig(\n    image=ImageConfig(\n        pixel_scale=0.263,       # arcsec/pixel (metacal scale=0.263)\n        size_x=48,\n        size_y=48,\n        n_objects=1,\n        fft_size=128,\n        noise=NoiseConfig(type=\"Gaussian\", sigma=1e-6),  # metacal noise=1e-6\n    ),\n    psf=PSFConfig(\n        type=\"Moffat\",\n        sigma=0.9,               # FWHM in arcsec (metacal psf_fwhm=0.9)\n        beta=2.5,                # metacal beta=2.5\n    ),\n    gal=GalaxyConfig(\n        type=\"Exponential\",      # metacal galsim.Exponential\n        flux=1.0,                # metacal default flux=1\n        half_light_radius=0.5,   # arcsec (metacal gal_hlr=0.5)\n        ellipticity=EllipticityConfig(type=\"E1E2\", e1=0.0, e2=0.0),\n        shear=ShearConfig(\n            type=\"G1G2\",\n            g1=DistributionConfig(type=\"Normal\", mean=0.0, sigma=0.05),\n            g2=DistributionConfig(type=\"Normal\", mean=0.0, sigma=0.05),\n        ),\n        position=PositionConfig(\n            type=\"Uniform\",\n            x_min=23.5, x_max=24.5,\n            y_min=23.5, y_max=24.5,\n        ),\n    ),\n    inference=InferenceConfig(\n        method=\"map\",\n        map_config=MAPConfig(num_steps=3000, learning_rate=0.005),\n        rng_seed=42,\n    ),\n)\n\nprint(f\"Image: {config.image.size_x}x{config.image.size_y} px, \"\n      f\"scale={config.image.pixel_scale}\\\"/px\")\nprint(f\"Galaxy: {config.gal.type}, flux={config.gal.flux}, \"\n      f\"hlr={config.gal.half_light_radius}\\\"\")\nprint(f\"PSF: {config.psf.type}, FWHM={config.psf.sigma}\\\", beta={config.psf.beta}\")\nprint(f\"Noise sigma: {config.image.noise.sigma}\")\nprint(f\"True shear: g1={G1_TRUE}, g2={G2_TRUE}\")\nprint(f\"Batch size: {N_BATCH}\")\nprint(f\"Inference method: {config.inference.method}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Generate Observations\n\nAll 10 galaxies share the same true shear but have independent (effectively zero) noise\nrealizations. We use `generate_biased_observation()` per realization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate N_BATCH independent observations with the same true shear\nseeds = list(range(100, 100 + N_BATCH))\nsim_results = []\nfor seed in seeds:\n    sim = generate_biased_observation(config, G1_TRUE, G2_TRUE, seed)\n    sim_results.append(sim)\n\nprint(f\"Generated {N_BATCH} observations\")\nprint(f\"Image shape: {sim_results[0].observation.image.shape}\")\nprint(f\"PSF type: {type(sim_results[0].observation.psf_model).__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize a few of the generated images\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor i, ax in enumerate(axes.flat):\n    im = ax.imshow(sim_results[i].observation.image, origin=\"lower\", cmap=\"gray_r\")\n    ax.set_title(f\"Galaxy {i}\", fontsize=10)\n    ax.set_xticks([])\n    ax.set_yticks([])\nfig.suptitle(\n    f\"Level 0: 10 Exponential galaxies, g1={G1_TRUE}, g2={G2_TRUE}, \"\n    f\"noise={config.image.noise.sigma}\",\n    fontsize=12,\n)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Run MAP Inference\n\nFor each realization, we build a model and run MAP estimation. Since Level 0\nis noiseless, MAP finds the maximum a posteriori point estimate which should\nmatch the truth exactly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run MAP inference for each realization\nscene = SceneBuilder(config)\nmodel_fn = scene.build_model()\n\nmap_cfg = config.inference.map_config\nprint(f\"Inference method: {config.inference.method}\")\nprint(f\"MAP: {map_cfg.num_steps} steps, lr={map_cfg.learning_rate}\")\n\nidata_list = []\nfor i, sim in enumerate(sim_results):\n    rng_key = jax.random.PRNGKey(config.inference.rng_seed + i)\n    engine = Inference(model=model_fn, config=config.inference)\n    idata = engine.run(\n        rng_key=rng_key,\n        observed_data=sim.observation.image,\n        extra_args={\"psf\": sim.observation.psf_model},\n    )\n    idata_list.append(idata)\n    g1_val = float(idata.posterior.g1.values.flatten()[0])\n    g2_val = float(idata.posterior.g2.values.flatten()[0])\n    print(f\"  Realization {i}: g1={g1_val:+.6f}, g2={g2_val:+.6f}\")\n\nprint(f\"\\nCompleted {N_BATCH} MAP estimations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Extract Estimates\n\nFor each realization, extract the MAP point estimates and check that they\nmatch the truth."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "run_ids = [f\"level0_{i:04d}\" for i in range(N_BATCH)]\n\nprint(f\"Extracted {N_BATCH} MAP estimates\")\nprint(f\"Example: {run_ids[0]}, posterior vars: {list(idata_list[0].posterior.data_vars)}\")\nprint(f\"  inference_method: {idata_list[0].posterior.attrs.get('inference_method')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Extract Diagnostics\n\nFor each realization, extract:\n- **Shear estimates**: MAP point estimate (mean = median = value, std = 0)\n- **Convergence diagnostics**: sentinel values for MAP (rhat=1, ess=1)\n\nLevel 0 acceptance criterion for MAP: the estimate should be very close to truth."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract results for all realizations\nresults = []\nfor run_id, single_idata in zip(run_ids, idata_list):\n    g1_est = extract_shear_estimates(single_idata, \"g1\")\n    g2_est = extract_shear_estimates(single_idata, \"g2\")\n    diag = extract_convergence_diagnostics(single_idata)\n    method = single_idata.posterior.attrs.get(\"inference_method\", \"nuts\")\n    passed = check_convergence(diag, ConvergenceThresholds(), method=method)\n    results.append({\n        \"run_id\": run_id,\n        \"g1_est\": g1_est,\n        \"g2_est\": g2_est,\n        \"diagnostics\": diag,\n        \"passed\": passed,\n    })\n\n# Summary table\nprint(f\"{'Run ID':<14} {'g1 estimate':>14} {'g2 estimate':>14} {'Pass':>5}\")\nprint(\"-\" * 55)\nfor r in results:\n    print(\n        f\"{r['run_id']:<14} \"\n        f\"{r['g1_est'].mean:>14.6f} \"\n        f\"{r['g2_est'].mean:>14.6f} \"\n        f\"{'OK' if r['passed'] else 'FAIL':>5}\"\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Acceptance Criteria Check\n\nFor Level 0 MAP, each realization must satisfy:\n1. MAP estimate close to truth (absolute offset $< 10^{-3}$)\n2. Convergence always passes for MAP (no sampling diagnostics)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MAX_ABS_OFFSET = 1e-3\n\nall_passed = True\nprint(\"Level 0 Acceptance Criteria (MAP)\")\nprint(\"=\" * 80)\n\nfor r in results:\n    run_id = r[\"run_id\"]\n    g1, g2 = r[\"g1_est\"], r[\"g2_est\"]\n\n    # For MAP: check absolute offset from truth\n    g1_offset = abs(g1.mean - G1_TRUE)\n    g2_offset = abs(g2.mean - G2_TRUE)\n    offset_ok = g1_offset < MAX_ABS_OFFSET and g2_offset < MAX_ABS_OFFSET\n\n    passed = offset_ok and r[\"passed\"]\n    all_passed = all_passed and passed\n\n    status = \"PASS\" if passed else \"FAIL\"\n    print(f\"\\n{run_id} [{status}]\")\n    print(f\"  g1: truth={G1_TRUE:+.4f}  MAP={g1.mean:+.6f}  \"\n          f\"|offset|={g1_offset:.2e}  {'ok' if g1_offset < MAX_ABS_OFFSET else 'FAIL'}\")\n    print(f\"  g2: truth={G2_TRUE:+.4f}  MAP={g2.mean:+.6f}  \"\n          f\"|offset|={g2_offset:.2e}  {'ok' if g2_offset < MAX_ABS_OFFSET else 'FAIL'}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Overall Level 0 result: {'ALL PASSED' if all_passed else 'SOME FAILED'}\")\nprint(f\"  {sum(1 for r in results if r['passed'])}/{len(results)} \"\n      f\"realizations passed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multiplicative Bias\n",
    "\n",
    "For each realization, compute the multiplicative bias:\n",
    "$$m = \\frac{\\bar{g}_{\\rm est}}{g_{\\rm true}} - 1$$\n",
    "\n",
    "At Level 0 (noiseless, self-consistent model), we expect $m \\approx 0$.\n",
    "\n",
    "Since $g_2^{\\rm true} = 0$ (matching metacal), we cannot compute $m$ for $g_2$\n",
    "(division by zero). Instead we report the additive residual $c_2 = \\bar{g}_2 - 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "bias_g1_list = []\n\nprint(f\"{'Run ID':<14} {'m(g1)':>12} {'c(g2)':>12}\")\nprint(\"-\" * 42)\n\nfor r in results:\n    b1 = compute_bias_single_point(G1_TRUE, r[\"g1_est\"].mean, r[\"g1_est\"].std, \"g1\")\n    bias_g1_list.append(b1)\n    # g2_true=0 so we report additive residual instead of multiplicative bias\n    c2 = r[\"g2_est\"].mean - G2_TRUE\n    print(f\"{r['run_id']:<14} {b1.m:>12.6f} {c2:>12.2e}\")\n\n# Ensemble average\nm_g1_vals = np.array([b.m for b in bias_g1_list])\nc_g2_vals = np.array([r[\"g2_est\"].mean - G2_TRUE for r in results])\nprint(\"-\" * 42)\nprint(f\"{'Ensemble mean':<14} {m_g1_vals.mean():>12.6f} {c_g2_vals.mean():>12.2e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect all g1, g2 MAP estimates\ng1_means = np.array([r[\"g1_est\"].mean for r in results])\ng2_means = np.array([r[\"g2_est\"].mean for r in results])\nindices = np.arange(N_BATCH)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# g1 estimates\naxes[0].scatter(indices, g1_means, marker=\"o\", s=60, color=\"steelblue\",\n                zorder=5, label=\"MAP estimate\")\naxes[0].axhline(G1_TRUE, color=\"red\", ls=\"--\", lw=2, label=f\"Truth = {G1_TRUE}\")\naxes[0].fill_between([-0.5, N_BATCH - 0.5],\n                     G1_TRUE - MAX_ABS_OFFSET,\n                     G1_TRUE + MAX_ABS_OFFSET,\n                     color=\"red\", alpha=0.1, label=f\"$\\\\pm${MAX_ABS_OFFSET}\")\naxes[0].set_xlabel(\"Realization\")\naxes[0].set_ylabel(\"$g_1$\")\naxes[0].set_title(\"$g_1$ Recovery (MAP)\")\naxes[0].legend(fontsize=9)\naxes[0].set_xticks(indices)\n\n# g2 estimates\naxes[1].scatter(indices, g2_means, marker=\"o\", s=60, color=\"coral\",\n                zorder=5, label=\"MAP estimate\")\naxes[1].axhline(G2_TRUE, color=\"red\", ls=\"--\", lw=2, label=f\"Truth = {G2_TRUE}\")\naxes[1].fill_between([-0.5, N_BATCH - 0.5],\n                     G2_TRUE - MAX_ABS_OFFSET,\n                     G2_TRUE + MAX_ABS_OFFSET,\n                     color=\"red\", alpha=0.1, label=f\"$\\\\pm${MAX_ABS_OFFSET}\")\naxes[1].set_xlabel(\"Realization\")\naxes[1].set_ylabel(\"$g_2$\")\naxes[1].set_title(\"$g_2$ Recovery (MAP)\")\naxes[1].legend(fontsize=9)\naxes[1].set_xticks(indices)\n\nfig.suptitle(\"Level 0: MAP Shear Recovery Across 10 Realizations\", fontsize=13)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bias plots: m(g1) and c(g2)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# g1: multiplicative bias\naxes[0].scatter(indices, m_g1_vals, marker=\"s\", s=60, color=\"steelblue\")\naxes[0].axhline(0, color=\"red\", ls=\"--\", lw=2, label=\"$m = 0$ (no bias)\")\naxes[0].axhline(m_g1_vals.mean(), color=\"green\", ls=\":\", lw=1.5,\n                label=f\"Mean $m$ = {m_g1_vals.mean():.2e}\")\naxes[0].set_xlabel(\"Realization\")\naxes[0].set_ylabel(\"$m_{g_1}$\")\naxes[0].set_title(\"Multiplicative Bias $g_1$\")\naxes[0].legend(fontsize=9)\naxes[0].set_xticks(indices)\n\n# g2: additive bias (g2_true = 0)\naxes[1].scatter(indices, c_g2_vals, marker=\"s\", s=60, color=\"coral\")\naxes[1].axhline(0, color=\"red\", ls=\"--\", lw=2, label=\"$c = 0$ (no bias)\")\naxes[1].axhline(c_g2_vals.mean(), color=\"green\", ls=\":\", lw=1.5,\n                label=f\"Mean $c$ = {c_g2_vals.mean():.2e}\")\naxes[1].set_xlabel(\"Realization\")\naxes[1].set_ylabel(\"$c_{g_2}$\")\naxes[1].set_title(\"Additive Bias $g_2$ ($g_2^{\\\\rm true} = 0$)\")\naxes[1].legend(fontsize=9)\naxes[1].set_xticks(indices)\n\nfig.suptitle(\"Level 0: Bias per Realization (MAP)\", fontsize=13)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# g1 vs g2 MAP estimates across all realizations\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.scatter(g1_means, g2_means, s=60, color=\"steelblue\", zorder=5, label=\"MAP estimates\")\nax.scatter([G1_TRUE], [G2_TRUE], c=\"red\", s=150, marker=\"*\",\n           zorder=10, label=\"Truth\")\nax.set_xlabel(\"$g_1$\")\nax.set_ylabel(\"$g_2$\")\nax.set_title(\"MAP Estimates: $g_1$ vs $g_2$\")\nax.legend()\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Summary\n\nThis Level 0 test validates that SHINE's forward model is **self-consistent**: when\nthe data is generated from the same model with no noise, the MAP estimate recovers\nthe true shear values with negligible bias.\n\nSince Level 0 is noiseless, MAP is the natural and fastest inference method --\nfull MCMC is unnecessary. For higher validation levels (Level 1+) with realistic\nnoise, NUTS or VI should be used instead."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary\nn_passed = sum(1 for r in results if r[\"passed\"])\n\nprint(\"Level 0 MAP Inference Summary\")\nprint(\"=\" * 50)\nprint(f\"  Batch size:           {N_BATCH}\")\nprint(f\"  True shear:           g1={G1_TRUE}, g2={G2_TRUE}\")\nprint(f\"  Inference method:     MAP\")\nprint(f\"  All passed:           {n_passed}/{N_BATCH}\")\nprint(f\"  Mean m(g1):           {m_g1_vals.mean():.2e}\")\nprint(f\"  Mean c(g2):           {c_g2_vals.mean():.2e}\")\nprint(f\"  Max |g1 offset|:      {max(abs(r['g1_est'].mean - G1_TRUE) for r in results):.2e}\")\nprint(f\"  Max |g2 offset|:      {max(abs(r['g2_est'].mean - G2_TRUE) for r in results):.2e}\")\nprint(\"=\" * 50)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}